{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ann_2014-01-01_2022-02-28\"\n",
    "df_dataset = pd.read_csv(f\"dataset/{dataset_name}.csv\", header=0, parse_dates=[\"timestamp\"], index_col=0)\n",
    "\n",
    "FEATURES = 3\n",
    "label_name = \"Eeg_label\"\n",
    "\n",
    "baseline = f\"{dataset_name}_{label_name}\"\n",
    "\n",
    "features = df_dataset.iloc[:,0:FEATURES].values\n",
    "label = df_dataset.loc[:, label_name].values.reshape(-1,1)\n",
    "\n",
    "x_train = features[0:int(len(features)*0.7)]\n",
    "x_valid = features[int(len(features)*0.7):int(len(features)*0.8)]\n",
    "x_test = features[int(len(features)*0.8):]\n",
    "\n",
    "y_train = label[0:int(len(label)*0.7)]\n",
    "y_valid = label[int(len(label)*0.7):int(len(label)*0.8)]\n",
    "y_test = label[int(len(label)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(FEATURES))\n",
    "ann_1 = tf.keras.layers.Dense(128, activation=\"relu\")(input_layer)\n",
    "ann_2 = tf.keras.layers.Dense(64, activation=\"relu\")(ann_1)\n",
    "ann_3 = tf.keras.layers.Dense(32, activation=\"relu\")(ann_2)\n",
    "output_layer = tf.keras.layers.Dense(1, activation=None)(ann_3)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 12422.7051\n",
      "Epoch 1: val_loss improved from inf to 8342.17480, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0001-8342.1748.h5\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 12398.9443 - val_loss: 8342.1748\n",
      "Epoch 2/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 8590.3877\n",
      "Epoch 2: val_loss did not improve from 8342.17480\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 8588.5488 - val_loss: 8569.3037\n",
      "Epoch 3/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 8231.6758\n",
      "Epoch 3: val_loss improved from 8342.17480 to 7020.81689, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0003-7020.8169.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8238.9053 - val_loss: 7020.8169\n",
      "Epoch 4/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 8110.9189\n",
      "Epoch 4: val_loss did not improve from 7020.81689\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8114.9678 - val_loss: 7147.3296\n",
      "Epoch 5/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 8090.2397\n",
      "Epoch 5: val_loss did not improve from 7020.81689\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8093.4414 - val_loss: 8214.4590\n",
      "Epoch 6/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 8091.9995\n",
      "Epoch 6: val_loss did not improve from 7020.81689\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8083.1133 - val_loss: 7071.2998\n",
      "Epoch 7/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 8030.2031\n",
      "Epoch 7: val_loss improved from 7020.81689 to 6892.02002, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0007-6892.0200.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8030.2031 - val_loss: 6892.0200\n",
      "Epoch 8/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 8022.1660\n",
      "Epoch 8: val_loss did not improve from 6892.02002\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 8026.3330 - val_loss: 8643.2090\n",
      "Epoch 9/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7980.7104\n",
      "Epoch 9: val_loss did not improve from 6892.02002\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7990.4995 - val_loss: 7626.5698\n",
      "Epoch 10/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7993.4053\n",
      "Epoch 10: val_loss improved from 6892.02002 to 6872.62842, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0010-6872.6284.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7998.1719 - val_loss: 6872.6284\n",
      "Epoch 11/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7969.0181\n",
      "Epoch 11: val_loss did not improve from 6872.62842\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7973.3296 - val_loss: 7781.4370\n",
      "Epoch 12/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7956.3267\n",
      "Epoch 12: val_loss did not improve from 6872.62842\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7956.3267 - val_loss: 8072.0059\n",
      "Epoch 13/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7974.5513\n",
      "Epoch 13: val_loss did not improve from 6872.62842\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7975.3594 - val_loss: 7805.9883\n",
      "Epoch 14/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7956.2827\n",
      "Epoch 14: val_loss improved from 6872.62842 to 6712.59570, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0014-6712.5957.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7952.4805 - val_loss: 6712.5957\n",
      "Epoch 15/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7939.6558\n",
      "Epoch 15: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7942.3125 - val_loss: 7203.6021\n",
      "Epoch 16/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7927.0596\n",
      "Epoch 16: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7930.0825 - val_loss: 7470.1216\n",
      "Epoch 17/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7920.6045\n",
      "Epoch 17: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7919.0605 - val_loss: 6886.5225\n",
      "Epoch 18/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7932.1323\n",
      "Epoch 18: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7925.6499 - val_loss: 7826.6426\n",
      "Epoch 19/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7930.8140\n",
      "Epoch 19: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7930.0845 - val_loss: 6757.9375\n",
      "Epoch 20/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7900.6797\n",
      "Epoch 20: val_loss did not improve from 6712.59570\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7900.6797 - val_loss: 6757.5132\n",
      "Epoch 21/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7890.1260\n",
      "Epoch 21: val_loss improved from 6712.59570 to 6703.61865, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0021-6703.6187.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7889.8115 - val_loss: 6703.6187\n",
      "Epoch 22/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7910.7793\n",
      "Epoch 22: val_loss improved from 6703.61865 to 6696.13574, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0022-6696.1357.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7908.3140 - val_loss: 6696.1357\n",
      "Epoch 23/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7909.0483\n",
      "Epoch 23: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7916.6123 - val_loss: 7079.1963\n",
      "Epoch 24/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7910.4751\n",
      "Epoch 24: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7911.0039 - val_loss: 6737.4004\n",
      "Epoch 25/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7887.7173\n",
      "Epoch 25: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7890.2891 - val_loss: 7061.3696\n",
      "Epoch 26/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7915.2324\n",
      "Epoch 26: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7907.9009 - val_loss: 7609.5640\n",
      "Epoch 27/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7903.8403\n",
      "Epoch 27: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7898.0312 - val_loss: 6809.4619\n",
      "Epoch 28/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7884.5073\n",
      "Epoch 28: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7882.4990 - val_loss: 6718.6216\n",
      "Epoch 29/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7865.2563\n",
      "Epoch 29: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7866.4839 - val_loss: 7173.3149\n",
      "Epoch 30/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7870.9482\n",
      "Epoch 30: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7874.7031 - val_loss: 7852.3193\n",
      "Epoch 31/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7866.4175\n",
      "Epoch 31: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7866.2295 - val_loss: 7358.6572\n",
      "Epoch 32/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7882.0142\n",
      "Epoch 32: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7876.4116 - val_loss: 7306.7534\n",
      "Epoch 33/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7906.9150\n",
      "Epoch 33: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7906.7275 - val_loss: 7040.2461\n",
      "Epoch 34/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7888.4644\n",
      "Epoch 34: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7882.1567 - val_loss: 6766.0298\n",
      "Epoch 35/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7860.5088\n",
      "Epoch 35: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7863.5234 - val_loss: 6871.9434\n",
      "Epoch 36/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7860.8403\n",
      "Epoch 36: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7854.4351 - val_loss: 6795.4092\n",
      "Epoch 37/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7849.5547\n",
      "Epoch 37: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7850.5337 - val_loss: 7004.1001\n",
      "Epoch 38/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7868.5469\n",
      "Epoch 38: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7858.3721 - val_loss: 7504.5776\n",
      "Epoch 39/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7841.5576\n",
      "Epoch 39: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7838.9185 - val_loss: 7885.0396\n",
      "Epoch 40/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7839.8682\n",
      "Epoch 40: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7845.0845 - val_loss: 6721.0161\n",
      "Epoch 41/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7854.0190\n",
      "Epoch 41: val_loss did not improve from 6696.13574\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7853.6875 - val_loss: 7044.6436\n",
      "Epoch 42/1000\n",
      "3391/3413 [============================>.] - ETA: 0s - loss: 7859.9380\n",
      "Epoch 42: val_loss improved from 6696.13574 to 6656.57520, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0042-6656.5752.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7852.6089 - val_loss: 6656.5752\n",
      "Epoch 43/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7824.6919\n",
      "Epoch 43: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7823.0059 - val_loss: 7008.3677\n",
      "Epoch 44/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7839.9575\n",
      "Epoch 44: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7839.1772 - val_loss: 7794.5376\n",
      "Epoch 45/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7825.0425\n",
      "Epoch 45: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 2ms/step - loss: 7832.7041 - val_loss: 7916.8184\n",
      "Epoch 46/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7831.5479\n",
      "Epoch 46: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7830.3267 - val_loss: 6796.8203\n",
      "Epoch 47/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7829.0938\n",
      "Epoch 47: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7829.0938 - val_loss: 7027.3730\n",
      "Epoch 48/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7824.5615\n",
      "Epoch 48: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7832.8721 - val_loss: 7362.8486\n",
      "Epoch 49/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7797.8682\n",
      "Epoch 49: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7807.4150 - val_loss: 6865.7300\n",
      "Epoch 50/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7808.0171\n",
      "Epoch 50: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7808.0171 - val_loss: 6679.4922\n",
      "Epoch 51/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7832.2915\n",
      "Epoch 51: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7831.9023 - val_loss: 7222.2202\n",
      "Epoch 52/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7800.5664\n",
      "Epoch 52: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7804.4131 - val_loss: 6870.6631\n",
      "Epoch 53/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7810.6880\n",
      "Epoch 53: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7819.3853 - val_loss: 6688.8613\n",
      "Epoch 54/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7821.7153\n",
      "Epoch 54: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7820.1050 - val_loss: 6795.8955\n",
      "Epoch 55/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7806.0117\n",
      "Epoch 55: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7807.8525 - val_loss: 7442.8374\n",
      "Epoch 56/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7811.7251\n",
      "Epoch 56: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7811.7378 - val_loss: 6764.8164\n",
      "Epoch 57/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7808.3750\n",
      "Epoch 57: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7811.5034 - val_loss: 6820.2642\n",
      "Epoch 58/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7786.0713\n",
      "Epoch 58: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7792.3335 - val_loss: 7034.3037\n",
      "Epoch 59/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7806.7837\n",
      "Epoch 59: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7809.1738 - val_loss: 7490.8335\n",
      "Epoch 60/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7810.6079\n",
      "Epoch 60: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7813.0952 - val_loss: 7222.2549\n",
      "Epoch 61/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7796.5278\n",
      "Epoch 61: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7804.6499 - val_loss: 7047.5659\n",
      "Epoch 62/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7801.4634\n",
      "Epoch 62: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7798.6509 - val_loss: 7163.9727\n",
      "Epoch 63/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7807.1001\n",
      "Epoch 63: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7806.8345 - val_loss: 7783.1270\n",
      "Epoch 64/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7781.2490\n",
      "Epoch 64: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7782.1772 - val_loss: 6737.5044\n",
      "Epoch 65/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7793.9263\n",
      "Epoch 65: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7788.2202 - val_loss: 6877.8491\n",
      "Epoch 66/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7787.9961\n",
      "Epoch 66: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7790.8882 - val_loss: 6910.1064\n",
      "Epoch 67/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7782.7358\n",
      "Epoch 67: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7784.5645 - val_loss: 7288.1436\n",
      "Epoch 68/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7800.3745\n",
      "Epoch 68: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7799.0000 - val_loss: 6863.5415\n",
      "Epoch 69/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7812.7812\n",
      "Epoch 69: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7811.5825 - val_loss: 7160.0371\n",
      "Epoch 70/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7817.3516\n",
      "Epoch 70: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7817.0835 - val_loss: 6745.4316\n",
      "Epoch 71/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7798.1392\n",
      "Epoch 71: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7795.5469 - val_loss: 6960.1392\n",
      "Epoch 72/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7783.2939\n",
      "Epoch 72: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7792.0059 - val_loss: 7431.7021\n",
      "Epoch 73/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7773.9604\n",
      "Epoch 73: val_loss did not improve from 6656.57520\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7778.8931 - val_loss: 7665.6577\n",
      "Epoch 74/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7815.4644\n",
      "Epoch 74: val_loss improved from 6656.57520 to 6623.24365, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0074-6623.2437.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7805.3989 - val_loss: 6623.2437\n",
      "Epoch 75/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7797.7783\n",
      "Epoch 75: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7794.3843 - val_loss: 7006.5195\n",
      "Epoch 76/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7799.9976\n",
      "Epoch 76: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7796.3979 - val_loss: 6829.6997\n",
      "Epoch 77/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7787.9561\n",
      "Epoch 77: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 2ms/step - loss: 7790.4673 - val_loss: 7714.3516\n",
      "Epoch 78/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7803.4175\n",
      "Epoch 78: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7799.8506 - val_loss: 7635.2188\n",
      "Epoch 79/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7792.1719\n",
      "Epoch 79: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7788.4780 - val_loss: 6637.1680\n",
      "Epoch 80/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7779.7607\n",
      "Epoch 80: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7781.7246 - val_loss: 6797.2510\n",
      "Epoch 81/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7766.2207\n",
      "Epoch 81: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7763.7622 - val_loss: 6846.0903\n",
      "Epoch 82/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7778.0522\n",
      "Epoch 82: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7778.0171 - val_loss: 6735.8242\n",
      "Epoch 83/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7758.5166\n",
      "Epoch 83: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7765.6777 - val_loss: 6954.1631\n",
      "Epoch 84/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7798.0225\n",
      "Epoch 84: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7794.0991 - val_loss: 6706.9946\n",
      "Epoch 85/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7772.3501\n",
      "Epoch 85: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7775.8174 - val_loss: 7043.3130\n",
      "Epoch 86/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7783.8960\n",
      "Epoch 86: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7781.6621 - val_loss: 6823.7222\n",
      "Epoch 87/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7762.5957\n",
      "Epoch 87: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7760.3716 - val_loss: 6718.9673\n",
      "Epoch 88/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7774.8560\n",
      "Epoch 88: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7779.0493 - val_loss: 7691.5552\n",
      "Epoch 89/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7772.4707\n",
      "Epoch 89: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7767.7026 - val_loss: 6632.1777\n",
      "Epoch 90/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7774.8706\n",
      "Epoch 90: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7771.6030 - val_loss: 6976.3755\n",
      "Epoch 91/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7772.9536\n",
      "Epoch 91: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7772.9492 - val_loss: 6944.5542\n",
      "Epoch 92/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7768.7407\n",
      "Epoch 92: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7770.8071 - val_loss: 6645.7729\n",
      "Epoch 93/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7774.3604\n",
      "Epoch 93: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7774.0752 - val_loss: 6914.4346\n",
      "Epoch 94/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7786.7671\n",
      "Epoch 94: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7778.5288 - val_loss: 7067.9932\n",
      "Epoch 95/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7764.2188\n",
      "Epoch 95: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7768.6748 - val_loss: 6916.1040\n",
      "Epoch 96/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7757.6948\n",
      "Epoch 96: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7764.7368 - val_loss: 6737.0859\n",
      "Epoch 97/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7767.7231\n",
      "Epoch 97: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7768.9023 - val_loss: 6784.1562\n",
      "Epoch 98/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7754.1255\n",
      "Epoch 98: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7761.0679 - val_loss: 6889.0298\n",
      "Epoch 99/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7765.6953\n",
      "Epoch 99: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7769.9951 - val_loss: 6808.1362\n",
      "Epoch 100/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7759.2490\n",
      "Epoch 100: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7761.0469 - val_loss: 6961.0103\n",
      "Epoch 101/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7771.8159\n",
      "Epoch 101: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7771.5117 - val_loss: 6762.5908\n",
      "Epoch 102/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7783.0479\n",
      "Epoch 102: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7776.0518 - val_loss: 6679.8965\n",
      "Epoch 103/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7761.8735\n",
      "Epoch 103: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7762.2417 - val_loss: 6857.9814\n",
      "Epoch 104/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7756.4473\n",
      "Epoch 104: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7755.6509 - val_loss: 6661.7754\n",
      "Epoch 105/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7768.4917\n",
      "Epoch 105: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7763.5981 - val_loss: 6771.5239\n",
      "Epoch 106/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7767.5029\n",
      "Epoch 106: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7766.8535 - val_loss: 7002.8994\n",
      "Epoch 107/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7749.2764\n",
      "Epoch 107: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7746.9243 - val_loss: 6940.5825\n",
      "Epoch 108/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7766.8809\n",
      "Epoch 108: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7763.4521 - val_loss: 7369.5454\n",
      "Epoch 109/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7765.6211\n",
      "Epoch 109: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7763.9258 - val_loss: 6666.6948\n",
      "Epoch 110/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7761.2559\n",
      "Epoch 110: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7760.3193 - val_loss: 6715.2095\n",
      "Epoch 111/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7756.7627\n",
      "Epoch 111: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7755.6470 - val_loss: 6956.5684\n",
      "Epoch 112/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7740.0952\n",
      "Epoch 112: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7738.8794 - val_loss: 7102.9531\n",
      "Epoch 113/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7749.6533\n",
      "Epoch 113: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7749.6533 - val_loss: 6677.1074\n",
      "Epoch 114/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7755.2886\n",
      "Epoch 114: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7750.4663 - val_loss: 7272.7256\n",
      "Epoch 115/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7768.1753\n",
      "Epoch 115: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7768.1753 - val_loss: 7185.7998\n",
      "Epoch 116/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7769.7612\n",
      "Epoch 116: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7766.7041 - val_loss: 6654.4668\n",
      "Epoch 117/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7750.3989\n",
      "Epoch 117: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7747.9873 - val_loss: 6843.8555\n",
      "Epoch 118/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7749.8027\n",
      "Epoch 118: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7757.6030 - val_loss: 7087.4639\n",
      "Epoch 119/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7750.5161\n",
      "Epoch 119: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7754.6079 - val_loss: 6988.3550\n",
      "Epoch 120/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7745.0757\n",
      "Epoch 120: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7744.9019 - val_loss: 6711.9673\n",
      "Epoch 121/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7751.6211\n",
      "Epoch 121: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7742.2817 - val_loss: 6848.0957\n",
      "Epoch 122/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7753.5576\n",
      "Epoch 122: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7750.7993 - val_loss: 7086.2949\n",
      "Epoch 123/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7743.2939\n",
      "Epoch 123: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7747.1655 - val_loss: 7102.4082\n",
      "Epoch 124/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7745.4233\n",
      "Epoch 124: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7747.6304 - val_loss: 6858.1411\n",
      "Epoch 125/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7738.3076\n",
      "Epoch 125: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7733.6919 - val_loss: 6755.4321\n",
      "Epoch 126/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7741.2969\n",
      "Epoch 126: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7741.4492 - val_loss: 7505.5986\n",
      "Epoch 127/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7740.9131\n",
      "Epoch 127: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7739.1431 - val_loss: 6747.9106\n",
      "Epoch 128/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7753.9082\n",
      "Epoch 128: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7753.6416 - val_loss: 6790.1475\n",
      "Epoch 129/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7741.1587\n",
      "Epoch 129: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7741.1587 - val_loss: 6860.4941\n",
      "Epoch 130/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7746.0928\n",
      "Epoch 130: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7748.4604 - val_loss: 6938.5757\n",
      "Epoch 131/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7744.6802\n",
      "Epoch 131: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7742.7925 - val_loss: 7036.8301\n",
      "Epoch 132/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7747.4404\n",
      "Epoch 132: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7747.1465 - val_loss: 6884.9702\n",
      "Epoch 133/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7747.5474\n",
      "Epoch 133: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7747.5474 - val_loss: 6757.8188\n",
      "Epoch 134/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7728.9429\n",
      "Epoch 134: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7728.1016 - val_loss: 6780.6484\n",
      "Epoch 135/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7734.2202\n",
      "Epoch 135: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7734.2202 - val_loss: 6729.2690\n",
      "Epoch 136/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7723.2837\n",
      "Epoch 136: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7723.2837 - val_loss: 7142.3325\n",
      "Epoch 137/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7742.5190\n",
      "Epoch 137: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7736.6147 - val_loss: 7489.2109\n",
      "Epoch 138/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7727.7290\n",
      "Epoch 138: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7733.4238 - val_loss: 6864.6543\n",
      "Epoch 139/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7734.3213\n",
      "Epoch 139: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7733.6123 - val_loss: 7282.4033\n",
      "Epoch 140/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7739.2744\n",
      "Epoch 140: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7739.2812 - val_loss: 7008.9985\n",
      "Epoch 141/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7736.4590\n",
      "Epoch 141: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7738.4004 - val_loss: 7184.3765\n",
      "Epoch 142/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7748.8789\n",
      "Epoch 142: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7750.2373 - val_loss: 6910.3633\n",
      "Epoch 143/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7742.5684\n",
      "Epoch 143: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7744.5938 - val_loss: 6962.1631\n",
      "Epoch 144/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7735.8027\n",
      "Epoch 144: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7739.1885 - val_loss: 7331.0532\n",
      "Epoch 145/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7741.8179\n",
      "Epoch 145: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7740.7373 - val_loss: 6632.1851\n",
      "Epoch 146/1000\n",
      "3392/3413 [============================>.] - ETA: 0s - loss: 7728.3857\n",
      "Epoch 146: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7727.6836 - val_loss: 7132.5078\n",
      "Epoch 147/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7737.5425\n",
      "Epoch 147: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7738.0684 - val_loss: 6705.6826\n",
      "Epoch 148/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7736.4751\n",
      "Epoch 148: val_loss did not improve from 6623.24365\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7730.0977 - val_loss: 7089.4795\n",
      "Epoch 149/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7725.7744\n",
      "Epoch 149: val_loss improved from 6623.24365 to 6592.09424, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0149-6592.0942.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7726.5688 - val_loss: 6592.0942\n",
      "Epoch 150/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7735.3989\n",
      "Epoch 150: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7728.4014 - val_loss: 6806.4614\n",
      "Epoch 151/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7720.8994\n",
      "Epoch 151: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7723.4941 - val_loss: 6882.4082\n",
      "Epoch 152/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7735.5645\n",
      "Epoch 152: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7730.9653 - val_loss: 7129.0020\n",
      "Epoch 153/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7727.1011\n",
      "Epoch 153: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7737.3755 - val_loss: 7154.1040\n",
      "Epoch 154/1000\n",
      "3392/3413 [============================>.] - ETA: 0s - loss: 7735.1680\n",
      "Epoch 154: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7735.2124 - val_loss: 6661.5728\n",
      "Epoch 155/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7727.1177\n",
      "Epoch 155: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7724.1138 - val_loss: 7031.3276\n",
      "Epoch 156/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7717.4297\n",
      "Epoch 156: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7717.4312 - val_loss: 6646.0239\n",
      "Epoch 157/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7732.9185\n",
      "Epoch 157: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7735.7710 - val_loss: 7132.2759\n",
      "Epoch 158/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7729.5933\n",
      "Epoch 158: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7730.3291 - val_loss: 7198.3770\n",
      "Epoch 159/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7727.2524\n",
      "Epoch 159: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7723.5674 - val_loss: 6707.4829\n",
      "Epoch 160/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7724.5820\n",
      "Epoch 160: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.9224 - val_loss: 6724.2266\n",
      "Epoch 161/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7740.4819\n",
      "Epoch 161: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7736.0508 - val_loss: 6604.8311\n",
      "Epoch 162/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7713.5737\n",
      "Epoch 162: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.2495 - val_loss: 6635.6577\n",
      "Epoch 163/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7720.8711\n",
      "Epoch 163: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7725.2144 - val_loss: 6644.4458\n",
      "Epoch 164/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7722.7905\n",
      "Epoch 164: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7722.7441 - val_loss: 6653.8452\n",
      "Epoch 165/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7738.7446\n",
      "Epoch 165: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7735.9819 - val_loss: 6713.1206\n",
      "Epoch 166/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7719.4268\n",
      "Epoch 166: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7719.8643 - val_loss: 7369.7061\n",
      "Epoch 167/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7718.0557\n",
      "Epoch 167: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7725.2280 - val_loss: 6636.4038\n",
      "Epoch 168/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7719.7446\n",
      "Epoch 168: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.1226 - val_loss: 7348.5620\n",
      "Epoch 169/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7722.1328\n",
      "Epoch 169: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7723.3647 - val_loss: 6622.3447\n",
      "Epoch 170/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7712.6274\n",
      "Epoch 170: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7713.7056 - val_loss: 6889.8486\n",
      "Epoch 171/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7732.7153\n",
      "Epoch 171: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7727.7480 - val_loss: 6669.1895\n",
      "Epoch 172/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7711.9819\n",
      "Epoch 172: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.1196 - val_loss: 6762.9429\n",
      "Epoch 173/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7730.8066\n",
      "Epoch 173: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7733.8101 - val_loss: 7107.6455\n",
      "Epoch 174/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7725.0806\n",
      "Epoch 174: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7724.1650 - val_loss: 6779.5752\n",
      "Epoch 175/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7715.4888\n",
      "Epoch 175: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7722.0742 - val_loss: 6734.4331\n",
      "Epoch 176/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7718.1133\n",
      "Epoch 176: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.1753 - val_loss: 6817.5015\n",
      "Epoch 177/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7719.4810\n",
      "Epoch 177: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7719.0693 - val_loss: 6698.3813\n",
      "Epoch 178/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7743.4194\n",
      "Epoch 178: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7739.6465 - val_loss: 6672.6309\n",
      "Epoch 179/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7729.0459\n",
      "Epoch 179: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7723.3809 - val_loss: 6802.3413\n",
      "Epoch 180/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7720.3135\n",
      "Epoch 180: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7720.3135 - val_loss: 6748.9536\n",
      "Epoch 181/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7719.4565\n",
      "Epoch 181: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7718.6558 - val_loss: 6769.0493\n",
      "Epoch 182/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7717.5210\n",
      "Epoch 182: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.3569 - val_loss: 6935.0522\n",
      "Epoch 183/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7719.1528\n",
      "Epoch 183: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7716.2051 - val_loss: 7393.1968\n",
      "Epoch 184/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7724.2319\n",
      "Epoch 184: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7720.0928 - val_loss: 6811.7109\n",
      "Epoch 185/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7717.9731\n",
      "Epoch 185: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7722.5391 - val_loss: 6677.2095\n",
      "Epoch 186/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7728.3833\n",
      "Epoch 186: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7726.7583 - val_loss: 6763.2720\n",
      "Epoch 187/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7730.0068\n",
      "Epoch 187: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7728.2803 - val_loss: 7118.0693\n",
      "Epoch 188/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7723.6245\n",
      "Epoch 188: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7719.3716 - val_loss: 6829.4609\n",
      "Epoch 189/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7725.7026\n",
      "Epoch 189: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.9932 - val_loss: 7371.4648\n",
      "Epoch 190/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7728.7065\n",
      "Epoch 190: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7727.7861 - val_loss: 6684.1924\n",
      "Epoch 191/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7720.3579\n",
      "Epoch 191: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7719.8545 - val_loss: 6799.4688\n",
      "Epoch 192/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7712.8857\n",
      "Epoch 192: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7711.4185 - val_loss: 6765.7505\n",
      "Epoch 193/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7727.2520\n",
      "Epoch 193: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7720.3911 - val_loss: 6851.7485\n",
      "Epoch 194/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7721.0732\n",
      "Epoch 194: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.5513 - val_loss: 6667.1260\n",
      "Epoch 195/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7722.5386\n",
      "Epoch 195: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7716.2251 - val_loss: 6801.3447\n",
      "Epoch 196/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7718.7832\n",
      "Epoch 196: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7722.7114 - val_loss: 6916.9570\n",
      "Epoch 197/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7728.0039\n",
      "Epoch 197: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7726.4644 - val_loss: 6836.2725\n",
      "Epoch 198/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7712.5918\n",
      "Epoch 198: val_loss did not improve from 6592.09424\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7714.8115 - val_loss: 6678.4272\n",
      "Epoch 199/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7719.2959\n",
      "Epoch 199: val_loss improved from 6592.09424 to 6588.21729, saving model to models/ann_2014-01-01_2022-02-28_Eeg_label\\ann-Eeg_label-0199-6588.2173.h5\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7715.5386 - val_loss: 6588.2173\n",
      "Epoch 200/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7716.2573\n",
      "Epoch 200: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7721.8955 - val_loss: 6680.3506\n",
      "Epoch 201/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7727.7485\n",
      "Epoch 201: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7723.2793 - val_loss: 6692.9458\n",
      "Epoch 202/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7710.1567\n",
      "Epoch 202: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7712.0708 - val_loss: 6698.1694\n",
      "Epoch 203/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7706.3428\n",
      "Epoch 203: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7706.6299 - val_loss: 6656.6128\n",
      "Epoch 204/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7716.7061\n",
      "Epoch 204: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7704.7573 - val_loss: 6659.3071\n",
      "Epoch 205/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7704.0654\n",
      "Epoch 205: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.3599 - val_loss: 6614.9268\n",
      "Epoch 206/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7714.8154\n",
      "Epoch 206: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.0845 - val_loss: 6680.5044\n",
      "Epoch 207/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7695.5439\n",
      "Epoch 207: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7710.2173 - val_loss: 7228.2935\n",
      "Epoch 208/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7708.7715\n",
      "Epoch 208: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7708.7715 - val_loss: 6782.1406\n",
      "Epoch 209/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7726.4658\n",
      "Epoch 209: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7724.1016 - val_loss: 6968.1758\n",
      "Epoch 210/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7696.4038\n",
      "Epoch 210: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7708.8892 - val_loss: 6757.4653\n",
      "Epoch 211/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7705.7612\n",
      "Epoch 211: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7705.0757 - val_loss: 6869.4634\n",
      "Epoch 212/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7712.4331\n",
      "Epoch 212: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7706.0884 - val_loss: 6638.4194\n",
      "Epoch 213/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7711.2661\n",
      "Epoch 213: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7710.4058 - val_loss: 6634.0640\n",
      "Epoch 214/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7708.2148\n",
      "Epoch 214: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7705.9741 - val_loss: 6700.1567\n",
      "Epoch 215/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7704.4028\n",
      "Epoch 215: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7702.6196 - val_loss: 6653.9751\n",
      "Epoch 216/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7713.9775\n",
      "Epoch 216: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7709.8633 - val_loss: 7050.3267\n",
      "Epoch 217/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7700.7090\n",
      "Epoch 217: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7700.7090 - val_loss: 7007.7354\n",
      "Epoch 218/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7710.6626\n",
      "Epoch 218: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7706.8628 - val_loss: 6856.8574\n",
      "Epoch 219/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7705.7017\n",
      "Epoch 219: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7701.2417 - val_loss: 7220.6064\n",
      "Epoch 220/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7704.9146\n",
      "Epoch 220: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7706.9404 - val_loss: 7202.0903\n",
      "Epoch 221/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7699.3555\n",
      "Epoch 221: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7699.3555 - val_loss: 6630.4785\n",
      "Epoch 222/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7705.4146\n",
      "Epoch 222: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7711.1538 - val_loss: 6671.1445\n",
      "Epoch 223/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7704.5117\n",
      "Epoch 223: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7704.7012 - val_loss: 7013.2710\n",
      "Epoch 224/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7719.6265\n",
      "Epoch 224: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7716.7969 - val_loss: 6887.2549\n",
      "Epoch 225/1000\n",
      "3392/3413 [============================>.] - ETA: 0s - loss: 7715.3018\n",
      "Epoch 225: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7717.7847 - val_loss: 6729.8872\n",
      "Epoch 226/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7698.6719\n",
      "Epoch 226: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7698.6719 - val_loss: 6756.7485\n",
      "Epoch 227/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7700.6836\n",
      "Epoch 227: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7704.2271 - val_loss: 6705.6450\n",
      "Epoch 228/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7705.5820\n",
      "Epoch 228: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7703.5078 - val_loss: 7150.2441\n",
      "Epoch 229/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7717.1001\n",
      "Epoch 229: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7713.8379 - val_loss: 6720.9282\n",
      "Epoch 230/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7716.4678\n",
      "Epoch 230: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7717.6895 - val_loss: 6611.3218\n",
      "Epoch 231/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7690.0459\n",
      "Epoch 231: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7694.2217 - val_loss: 7619.9194\n",
      "Epoch 232/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7700.2427\n",
      "Epoch 232: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7701.0762 - val_loss: 6823.3613\n",
      "Epoch 233/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7716.8271\n",
      "Epoch 233: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.2529 - val_loss: 6700.3066\n",
      "Epoch 234/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7713.3447\n",
      "Epoch 234: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7710.4175 - val_loss: 6734.9722\n",
      "Epoch 235/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7704.1904\n",
      "Epoch 235: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7701.6265 - val_loss: 7208.7207\n",
      "Epoch 236/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7698.1035\n",
      "Epoch 236: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7700.3154 - val_loss: 6980.4458\n",
      "Epoch 237/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7709.1489\n",
      "Epoch 237: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7713.1421 - val_loss: 8150.3081\n",
      "Epoch 238/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7699.9468\n",
      "Epoch 238: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7700.1621 - val_loss: 6732.8037\n",
      "Epoch 239/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7696.4668\n",
      "Epoch 239: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7694.2070 - val_loss: 7318.7134\n",
      "Epoch 240/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7703.6235\n",
      "Epoch 240: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7703.6235 - val_loss: 6894.0527\n",
      "Epoch 241/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7695.5635\n",
      "Epoch 241: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7697.3267 - val_loss: 6686.0889\n",
      "Epoch 242/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7703.0186\n",
      "Epoch 242: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7705.8086 - val_loss: 6959.7368\n",
      "Epoch 243/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7697.0264\n",
      "Epoch 243: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7698.6006 - val_loss: 7054.7959\n",
      "Epoch 244/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7714.5244\n",
      "Epoch 244: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7711.3413 - val_loss: 6734.7476\n",
      "Epoch 245/1000\n",
      "3392/3413 [============================>.] - ETA: 0s - loss: 7707.9497\n",
      "Epoch 245: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7700.7881 - val_loss: 6735.1084\n",
      "Epoch 246/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7692.1685\n",
      "Epoch 246: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7696.4683 - val_loss: 6959.6196\n",
      "Epoch 247/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7713.9600\n",
      "Epoch 247: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7714.5664 - val_loss: 6904.0366\n",
      "Epoch 248/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7713.4077\n",
      "Epoch 248: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7705.8823 - val_loss: 6641.4434\n",
      "Epoch 249/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7707.3169\n",
      "Epoch 249: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7703.5630 - val_loss: 6623.9165\n",
      "Epoch 250/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7698.7949\n",
      "Epoch 250: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7691.5430 - val_loss: 7075.6875\n",
      "Epoch 251/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7692.9878\n",
      "Epoch 251: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7691.2251 - val_loss: 7274.5103\n",
      "Epoch 252/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7691.1782\n",
      "Epoch 252: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7694.3408 - val_loss: 6817.7124\n",
      "Epoch 253/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7702.0522\n",
      "Epoch 253: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7697.1582 - val_loss: 6720.0547\n",
      "Epoch 254/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7693.5728\n",
      "Epoch 254: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7693.5181 - val_loss: 6738.4614\n",
      "Epoch 255/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7702.0259\n",
      "Epoch 255: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7700.3838 - val_loss: 6665.4639\n",
      "Epoch 256/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7693.2310\n",
      "Epoch 256: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7691.8232 - val_loss: 6760.0366\n",
      "Epoch 257/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7701.1328\n",
      "Epoch 257: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7700.5259 - val_loss: 6678.1035\n",
      "Epoch 258/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7700.4253\n",
      "Epoch 258: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7699.6343 - val_loss: 6668.4136\n",
      "Epoch 259/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7688.0449\n",
      "Epoch 259: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7690.1104 - val_loss: 7610.2637\n",
      "Epoch 260/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7680.9316\n",
      "Epoch 260: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7681.5269 - val_loss: 6997.2778\n",
      "Epoch 261/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7689.1274\n",
      "Epoch 261: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7702.4175 - val_loss: 6689.8936\n",
      "Epoch 262/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7696.2373\n",
      "Epoch 262: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7694.4189 - val_loss: 6629.2803\n",
      "Epoch 263/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7681.0664\n",
      "Epoch 263: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7694.0415 - val_loss: 6821.6230\n",
      "Epoch 264/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7715.6724\n",
      "Epoch 264: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7707.6206 - val_loss: 6595.9863\n",
      "Epoch 265/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7682.5239\n",
      "Epoch 265: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7686.9106 - val_loss: 7083.1313\n",
      "Epoch 266/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7703.6694\n",
      "Epoch 266: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7698.1782 - val_loss: 6887.1318\n",
      "Epoch 267/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7696.5015\n",
      "Epoch 267: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7694.7554 - val_loss: 6824.3369\n",
      "Epoch 268/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7690.3975\n",
      "Epoch 268: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7690.6357 - val_loss: 6805.2881\n",
      "Epoch 269/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7693.4370\n",
      "Epoch 269: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7692.7817 - val_loss: 7068.9351\n",
      "Epoch 270/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7699.5273\n",
      "Epoch 270: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7696.0913 - val_loss: 6734.6567\n",
      "Epoch 271/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7687.4648\n",
      "Epoch 271: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7684.8105 - val_loss: 6884.2646\n",
      "Epoch 272/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7690.0332\n",
      "Epoch 272: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7688.4033 - val_loss: 6826.0156\n",
      "Epoch 273/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7685.9712\n",
      "Epoch 273: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7687.2051 - val_loss: 6723.8101\n",
      "Epoch 274/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7703.4766\n",
      "Epoch 274: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7700.7920 - val_loss: 6793.1089\n",
      "Epoch 275/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7695.6084\n",
      "Epoch 275: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7692.2422 - val_loss: 6740.8306\n",
      "Epoch 276/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7678.7090\n",
      "Epoch 276: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 12s 3ms/step - loss: 7685.4165 - val_loss: 6775.0020\n",
      "Epoch 277/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7693.3882\n",
      "Epoch 277: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7690.3374 - val_loss: 6658.3306\n",
      "Epoch 278/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7706.3760\n",
      "Epoch 278: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7703.6079 - val_loss: 6700.9038\n",
      "Epoch 279/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7692.6841\n",
      "Epoch 279: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7688.1982 - val_loss: 7110.0820\n",
      "Epoch 280/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7701.4199\n",
      "Epoch 280: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7699.6577 - val_loss: 6662.3242\n",
      "Epoch 281/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7687.0938\n",
      "Epoch 281: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7694.5488 - val_loss: 6663.0508\n",
      "Epoch 282/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7698.8271\n",
      "Epoch 282: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7697.7793 - val_loss: 6868.5103\n",
      "Epoch 283/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7688.5708\n",
      "Epoch 283: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7685.6689 - val_loss: 6766.1553\n",
      "Epoch 284/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7675.9639\n",
      "Epoch 284: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7674.7861 - val_loss: 6742.8281\n",
      "Epoch 285/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7691.6802\n",
      "Epoch 285: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7695.2485 - val_loss: 7362.5625\n",
      "Epoch 286/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7678.3081\n",
      "Epoch 286: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7677.8809 - val_loss: 6938.9502\n",
      "Epoch 287/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7684.8369\n",
      "Epoch 287: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7684.8369 - val_loss: 6728.1948\n",
      "Epoch 288/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7695.4595\n",
      "Epoch 288: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7695.8276 - val_loss: 6724.0415\n",
      "Epoch 289/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7700.9990\n",
      "Epoch 289: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7692.4893 - val_loss: 6647.4639\n",
      "Epoch 290/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7685.0327\n",
      "Epoch 290: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7681.5220 - val_loss: 7147.6226\n",
      "Epoch 291/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7682.6270\n",
      "Epoch 291: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7678.4517 - val_loss: 6980.6221\n",
      "Epoch 292/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7685.8730\n",
      "Epoch 292: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7685.7271 - val_loss: 6615.5723\n",
      "Epoch 293/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7678.9907\n",
      "Epoch 293: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7678.4971 - val_loss: 6679.0879\n",
      "Epoch 294/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7685.9917\n",
      "Epoch 294: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7679.4727 - val_loss: 6683.4648\n",
      "Epoch 295/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7691.2280\n",
      "Epoch 295: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7690.5317 - val_loss: 6750.6001\n",
      "Epoch 296/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7682.0166\n",
      "Epoch 296: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7683.2861 - val_loss: 6738.0293\n",
      "Epoch 297/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7679.9302\n",
      "Epoch 297: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7678.1108 - val_loss: 6785.9595\n",
      "Epoch 298/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7679.6436\n",
      "Epoch 298: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7681.0581 - val_loss: 6641.9321\n",
      "Epoch 299/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7684.2129\n",
      "Epoch 299: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7685.3926 - val_loss: 7145.3945\n",
      "Epoch 300/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7671.8770\n",
      "Epoch 300: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7671.8770 - val_loss: 6741.7354\n",
      "Epoch 301/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7668.4243\n",
      "Epoch 301: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7673.9595 - val_loss: 6680.4526\n",
      "Epoch 302/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7681.1519\n",
      "Epoch 302: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7682.9023 - val_loss: 6641.9971\n",
      "Epoch 303/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7687.6421\n",
      "Epoch 303: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7680.8418 - val_loss: 6621.1294\n",
      "Epoch 304/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7689.0908\n",
      "Epoch 304: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7689.0908 - val_loss: 6610.9697\n",
      "Epoch 305/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7687.2334\n",
      "Epoch 305: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7685.2319 - val_loss: 6630.3105\n",
      "Epoch 306/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7676.9580\n",
      "Epoch 306: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7680.8408 - val_loss: 6771.3027\n",
      "Epoch 307/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7677.8125\n",
      "Epoch 307: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7682.0640 - val_loss: 6731.4302\n",
      "Epoch 308/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7672.2339\n",
      "Epoch 308: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7669.5771 - val_loss: 7096.0171\n",
      "Epoch 309/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7666.2759\n",
      "Epoch 309: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7674.1865 - val_loss: 6901.5098\n",
      "Epoch 310/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7686.3867\n",
      "Epoch 310: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7686.8179 - val_loss: 7121.0249\n",
      "Epoch 311/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7671.6758\n",
      "Epoch 311: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7671.0303 - val_loss: 6954.4819\n",
      "Epoch 312/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7679.4146\n",
      "Epoch 312: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7688.6523 - val_loss: 6616.0396\n",
      "Epoch 313/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7673.3672\n",
      "Epoch 313: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7676.2026 - val_loss: 6705.8516\n",
      "Epoch 314/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7690.2710\n",
      "Epoch 314: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7690.2988 - val_loss: 6682.7417\n",
      "Epoch 315/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7680.3833\n",
      "Epoch 315: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7680.0884 - val_loss: 6656.0732\n",
      "Epoch 316/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7684.7725\n",
      "Epoch 316: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7681.5225 - val_loss: 6952.3926\n",
      "Epoch 317/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7665.3066\n",
      "Epoch 317: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7665.0938 - val_loss: 6763.8730\n",
      "Epoch 318/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7669.2104\n",
      "Epoch 318: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7669.9683 - val_loss: 7619.8647\n",
      "Epoch 319/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7671.9629\n",
      "Epoch 319: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7675.7344 - val_loss: 6845.0825\n",
      "Epoch 320/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7663.4761\n",
      "Epoch 320: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7667.1504 - val_loss: 6893.9126\n",
      "Epoch 321/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7692.7256\n",
      "Epoch 321: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7692.6543 - val_loss: 6876.2734\n",
      "Epoch 322/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7668.6328\n",
      "Epoch 322: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7674.0054 - val_loss: 6702.2827\n",
      "Epoch 323/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7676.6465\n",
      "Epoch 323: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7680.9395 - val_loss: 7231.7769\n",
      "Epoch 324/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7672.7119\n",
      "Epoch 324: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7672.7119 - val_loss: 6689.2095\n",
      "Epoch 325/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7677.3286\n",
      "Epoch 325: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7673.5215 - val_loss: 6723.5205\n",
      "Epoch 326/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7678.3374\n",
      "Epoch 326: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7677.5244 - val_loss: 6989.8843\n",
      "Epoch 327/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7681.7407\n",
      "Epoch 327: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7677.2124 - val_loss: 7404.0156\n",
      "Epoch 328/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7680.2788\n",
      "Epoch 328: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7679.4736 - val_loss: 6633.0098\n",
      "Epoch 329/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7689.1177\n",
      "Epoch 329: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7689.1177 - val_loss: 6971.3154\n",
      "Epoch 330/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7677.7183\n",
      "Epoch 330: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7665.8325 - val_loss: 6635.9805\n",
      "Epoch 331/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7668.7837\n",
      "Epoch 331: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7670.6929 - val_loss: 6815.2637\n",
      "Epoch 332/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7683.4888\n",
      "Epoch 332: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7684.2739 - val_loss: 6684.9932\n",
      "Epoch 333/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7671.3447\n",
      "Epoch 333: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7666.3535 - val_loss: 6621.9507\n",
      "Epoch 334/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7667.9297\n",
      "Epoch 334: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7675.9521 - val_loss: 7452.5552\n",
      "Epoch 335/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7663.7236\n",
      "Epoch 335: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7664.0967 - val_loss: 6732.8926\n",
      "Epoch 336/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7664.8057\n",
      "Epoch 336: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7663.8794 - val_loss: 6882.6592\n",
      "Epoch 337/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7683.7358\n",
      "Epoch 337: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7681.5063 - val_loss: 6668.9458\n",
      "Epoch 338/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7659.5845\n",
      "Epoch 338: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7659.6450 - val_loss: 6791.5210\n",
      "Epoch 339/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7674.2261\n",
      "Epoch 339: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7673.4419 - val_loss: 6875.5938\n",
      "Epoch 340/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7664.6919\n",
      "Epoch 340: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7665.3379 - val_loss: 6859.5845\n",
      "Epoch 341/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7678.1611\n",
      "Epoch 341: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7677.5293 - val_loss: 6880.3774\n",
      "Epoch 342/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7668.3179\n",
      "Epoch 342: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7669.1226 - val_loss: 7201.1548\n",
      "Epoch 343/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7671.6250\n",
      "Epoch 343: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7672.0767 - val_loss: 6753.1650\n",
      "Epoch 344/1000\n",
      "3393/3413 [============================>.] - ETA: 0s - loss: 7679.2427\n",
      "Epoch 344: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7672.5493 - val_loss: 6805.7759\n",
      "Epoch 345/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7673.4360\n",
      "Epoch 345: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7668.4707 - val_loss: 6655.7998\n",
      "Epoch 346/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7674.5249\n",
      "Epoch 346: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7673.9102 - val_loss: 6651.9292\n",
      "Epoch 347/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7680.2295\n",
      "Epoch 347: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7676.5117 - val_loss: 6777.5391\n",
      "Epoch 348/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7655.2993\n",
      "Epoch 348: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7661.4487 - val_loss: 6927.6099\n",
      "Epoch 349/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7665.4800\n",
      "Epoch 349: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 2ms/step - loss: 7665.4800 - val_loss: 6729.9199\n",
      "Epoch 350/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7677.4956\n",
      "Epoch 350: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7677.1411 - val_loss: 7452.8535\n",
      "Epoch 351/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7658.0640\n",
      "Epoch 351: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7662.7598 - val_loss: 8107.2368\n",
      "Epoch 352/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7673.9404\n",
      "Epoch 352: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7673.2544 - val_loss: 6986.1816\n",
      "Epoch 353/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7664.3193\n",
      "Epoch 353: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7662.6426 - val_loss: 6751.5645\n",
      "Epoch 354/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7661.5298\n",
      "Epoch 354: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7659.0068 - val_loss: 6932.7407\n",
      "Epoch 355/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7662.4946\n",
      "Epoch 355: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7666.0269 - val_loss: 6759.9395\n",
      "Epoch 356/1000\n",
      "3408/3413 [============================>.] - ETA: 0s - loss: 7662.9155\n",
      "Epoch 356: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7663.6875 - val_loss: 7344.4253\n",
      "Epoch 357/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7681.5747\n",
      "Epoch 357: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7677.0356 - val_loss: 6795.5439\n",
      "Epoch 358/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7662.3535\n",
      "Epoch 358: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7662.3535 - val_loss: 6792.8511\n",
      "Epoch 359/1000\n",
      "3394/3413 [============================>.] - ETA: 0s - loss: 7653.4497\n",
      "Epoch 359: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7670.0957 - val_loss: 7489.4810\n",
      "Epoch 360/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7661.1260\n",
      "Epoch 360: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7660.2202 - val_loss: 6789.7134\n",
      "Epoch 361/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7660.8784\n",
      "Epoch 361: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7661.9834 - val_loss: 6696.1821\n",
      "Epoch 362/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7643.9082\n",
      "Epoch 362: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7648.1191 - val_loss: 6849.0659\n",
      "Epoch 363/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7671.4146\n",
      "Epoch 363: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7669.4404 - val_loss: 6916.2070\n",
      "Epoch 364/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7667.9160\n",
      "Epoch 364: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7664.8760 - val_loss: 6885.0586\n",
      "Epoch 365/1000\n",
      "3396/3413 [============================>.] - ETA: 0s - loss: 7647.9312\n",
      "Epoch 365: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7653.7246 - val_loss: 6814.1724\n",
      "Epoch 366/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7670.5537\n",
      "Epoch 366: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7670.4619 - val_loss: 6831.2168\n",
      "Epoch 367/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7669.6367\n",
      "Epoch 367: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7669.6626 - val_loss: 7581.6924\n",
      "Epoch 368/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7659.8198\n",
      "Epoch 368: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7661.4814 - val_loss: 6684.9521\n",
      "Epoch 369/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7679.6807\n",
      "Epoch 369: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 8s 2ms/step - loss: 7675.8149 - val_loss: 7282.6934\n",
      "Epoch 370/1000\n",
      "3402/3413 [============================>.] - ETA: 0s - loss: 7667.3394\n",
      "Epoch 370: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7662.8213 - val_loss: 6780.0610\n",
      "Epoch 371/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7669.4019\n",
      "Epoch 371: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7673.2202 - val_loss: 7054.9287\n",
      "Epoch 372/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7681.5386\n",
      "Epoch 372: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7678.1689 - val_loss: 6974.8486\n",
      "Epoch 373/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7672.7983\n",
      "Epoch 373: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7674.6133 - val_loss: 6705.5317\n",
      "Epoch 374/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7657.7061\n",
      "Epoch 374: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7655.5083 - val_loss: 6778.6240\n",
      "Epoch 375/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7662.4995\n",
      "Epoch 375: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7658.4365 - val_loss: 6882.6519\n",
      "Epoch 376/1000\n",
      "3400/3413 [============================>.] - ETA: 0s - loss: 7652.8760\n",
      "Epoch 376: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7656.3271 - val_loss: 6698.7358\n",
      "Epoch 377/1000\n",
      "3404/3413 [============================>.] - ETA: 0s - loss: 7668.4790\n",
      "Epoch 377: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 9s 3ms/step - loss: 7670.3228 - val_loss: 6741.0576\n",
      "Epoch 378/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7672.5327\n",
      "Epoch 378: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7667.5732 - val_loss: 6998.1636\n",
      "Epoch 379/1000\n",
      "3407/3413 [============================>.] - ETA: 0s - loss: 7662.9077\n",
      "Epoch 379: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7665.5737 - val_loss: 6795.7241\n",
      "Epoch 380/1000\n",
      "3413/3413 [==============================] - ETA: 0s - loss: 7661.2998\n",
      "Epoch 380: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7661.2998 - val_loss: 6960.3911\n",
      "Epoch 381/1000\n",
      "3398/3413 [============================>.] - ETA: 0s - loss: 7673.9971\n",
      "Epoch 381: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7667.8584 - val_loss: 6950.5654\n",
      "Epoch 382/1000\n",
      "3397/3413 [============================>.] - ETA: 0s - loss: 7655.0977\n",
      "Epoch 382: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7657.4131 - val_loss: 6952.9487\n",
      "Epoch 383/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7663.9365\n",
      "Epoch 383: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7662.4995 - val_loss: 6722.7217\n",
      "Epoch 384/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7660.9844\n",
      "Epoch 384: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7659.8530 - val_loss: 7490.0156\n",
      "Epoch 385/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7670.6792\n",
      "Epoch 385: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7669.6938 - val_loss: 6873.5625\n",
      "Epoch 386/1000\n",
      "3406/3413 [============================>.] - ETA: 0s - loss: 7663.4702\n",
      "Epoch 386: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7663.8887 - val_loss: 6987.3745\n",
      "Epoch 387/1000\n",
      "3410/3413 [============================>.] - ETA: 0s - loss: 7662.5127\n",
      "Epoch 387: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7664.1992 - val_loss: 6686.5488\n",
      "Epoch 388/1000\n",
      "3405/3413 [============================>.] - ETA: 0s - loss: 7654.9370\n",
      "Epoch 388: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7652.9033 - val_loss: 6814.7734\n",
      "Epoch 389/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7653.1133\n",
      "Epoch 389: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7652.5381 - val_loss: 6912.1113\n",
      "Epoch 390/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7649.6631\n",
      "Epoch 390: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7649.4038 - val_loss: 6658.9429\n",
      "Epoch 391/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7647.2759\n",
      "Epoch 391: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7650.3105 - val_loss: 6921.9917\n",
      "Epoch 392/1000\n",
      "3411/3413 [============================>.] - ETA: 0s - loss: 7647.2710\n",
      "Epoch 392: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7645.9463 - val_loss: 7042.0366\n",
      "Epoch 393/1000\n",
      "3401/3413 [============================>.] - ETA: 0s - loss: 7653.5308\n",
      "Epoch 393: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7656.1968 - val_loss: 7008.9248\n",
      "Epoch 394/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7652.6636\n",
      "Epoch 394: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7650.4761 - val_loss: 6866.8970\n",
      "Epoch 395/1000\n",
      "3412/3413 [============================>.] - ETA: 0s - loss: 7660.0850\n",
      "Epoch 395: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7660.6147 - val_loss: 6744.5190\n",
      "Epoch 396/1000\n",
      "3395/3413 [============================>.] - ETA: 0s - loss: 7659.0469\n",
      "Epoch 396: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 11s 3ms/step - loss: 7651.3979 - val_loss: 7392.1133\n",
      "Epoch 397/1000\n",
      "3403/3413 [============================>.] - ETA: 0s - loss: 7658.3242\n",
      "Epoch 397: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7658.8872 - val_loss: 6917.6753\n",
      "Epoch 398/1000\n",
      "3399/3413 [============================>.] - ETA: 0s - loss: 7653.0723\n",
      "Epoch 398: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7654.4956 - val_loss: 6843.5146\n",
      "Epoch 399/1000\n",
      "3409/3413 [============================>.] - ETA: 0s - loss: 7655.9756\n",
      "Epoch 399: val_loss did not improve from 6588.21729\n",
      "3413/3413 [==============================] - 10s 3ms/step - loss: 7655.5557 - val_loss: 6734.0503\n",
      "Epoch 399: early stopping\n"
     ]
    }
   ],
   "source": [
    "PATH_SAVE = f\"models/{baseline}\"\n",
    "if not os.path.exists(PATH_SAVE):\n",
    "    os.makedirs(PATH_SAVE)\n",
    "\n",
    "checkpoint_model = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_SAVE + \"/ann-\" + label_name + \"-{epoch:04d}-{val_loss:.04f}.h5\",\n",
    "                                                      monitor='val_loss',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_weights_only=False,\n",
    "                                                      mode='auto',\n",
    "                                                      save_freq='epoch')\n",
    "stopping_criterion = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=200,\n",
    "                                                      verbose=1,\n",
    "                                                      mode='auto')\n",
    "\n",
    "history_model = model.fit(x = x_train, y = y_train, epochs=1000, batch_size=32, validation_data=(x_valid, y_valid),\n",
    "                          callbacks=[checkpoint_model, stopping_criterion])\n",
    "\n",
    "json.dump(history_model.history, open(f\"history/{baseline}.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22c84f1e040>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history_data = json.load(open(f\"history/{baseline}.json\", 'r'))\n",
    "plt.plot(history_data[\"loss\"])\n",
    "plt.plot(history_data[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da625c08aacc218fed67a5c400603b447ac5e0e7a22a28ba81b51d825a294cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf2.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
